<!doctype html>
<html lang="en">

<head>
	<meta charset="utf-8">

	<title>Deep reinforcement learning for flow control</title>

	<meta name="description" content="nhr4ces_workshop">
	<meta name="author" content="Andre Weiner">

	<meta name="apple-mobile-web-app-capable" content="yes">
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

	<meta name="viewport" content="width=device-width, initial-scale=1.0">

	<link rel="stylesheet" href="css/reset.css">
	<link rel="stylesheet" href="css/reveal.css">
	<link rel="stylesheet" href="css/theme/white.css" id="theme">

	<!-- Theme used for syntax highlighting of code -->
	<link rel="stylesheet" href="lib/css/monokai.css">

	<!-- Printing and PDF exports -->
	<script>
		var link = document.createElement('link');
		link.rel = 'stylesheet';
		link.type = 'text/css';
		link.href = window.location.search.match(/print-pdf/gi) ? 'css/print/pdf.css' : 'css/print/paper.css';
		document.getElementsByTagName('head')[0].appendChild(link);
	</script>

	<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
</head>

<style>
	.row_img {
		display: table;
	}

	.column_img {
		display: table-cell;
		vertical-align: middle;
		text-align: center;
		width: 50%;
	}

	.column_five {
		display: table-cell;
		vertical-align: middle;
		text-align: center;
		width: 20%;
	}

	.content_img {
		display: inline-block;
	}

	.reveal .slide-number {
		position: absolute;
		display: block;
		left: 8px;
		bottom: 8px;
		height: 48px;
		width: 50px;
		z-index: 31;
		font-family: Helvetica, sans-serif;
		font-size: 32px;
		line-height: 1;
		color: #000000;
		background-color: rgba(0, 0, 0, 0);
		padding: 5px;
	}
</style>

<body>

	<div class="reveal" data-video="small top-right">

		<!-- Any section element inside of this container is displayed as a slide -->
		<div class="slides">
			<!-- title slide -->
			<section data-video="small top-right">
				<h3>Deep reinforcement learning for flow control</h3>
				<p>
					<small><u>Andre Weiner,</u> Tom Krogmann, Janis Geise</small><br>
					<small>TU Braunschweig, <a href="https://www.tu-braunschweig.de/en/ism">Institute of Fluid
							Mechanics</a></small>
					<style>
						.row_logo {
							display: table;
						}

						.column_logo {
							display: table-cell;
							vertical-align: middle;
							text-align: center;
							width: 50%;
						}

						.content_logo {
							display: inline-block;
						}
					</style>
				<div class="row_logo">
					<div class="column_logo">
						<div class="content_logo">
							<img src="images/tubs_logo.png" alt="tubs_logo"
								style="background:none; border:none; box-shadow:none;">
						</div>
					</div>
					<div class="column_logo">
						<div class="content_logo">
							<table>
								<tr>
									<td style="border: 0; padding-right: 5px;">&#9993;</th>
									<td style="border: 0; padding-right: 20px;"><a
											href="mailto:a.weiner@tu-braunschweig.de">Mail</a></th>
									<td style="border: 0; padding-right: 5px;">&#9741;</td>
									<td style="border: 0; padding-right: 0px;"><a
											href="https://www.linkedin.com/in/andre-weiner-a79752133/">LinkedIn</a></td>
								</tr>
								<tr>
									<td style="border: 0; padding-right: 5px;">&#9997;</td>
									<td style="border: 0; padding-right: 20px;"><a href="https://ml-cfd.com/">Blog</a>
									</td>
									<td style="border: 0; padding-right: 5px;">&#10026;</td>
									<td style="border: 0; padding-right: 0px;"><a
											href="https://github.com/AndreWeiner">Github</a></td>
								</tr>
							</table>
						</div>
					</div>
				</div>
			</section>

			<!-- outline -->
			<section>
				<h2>Outline</h2>
				<ol>
					<li>Closed-loop active flow control</li>
					<li>Reinforcement learning basics</li>
					<li>A close look at PPO</li>
					<li>Optimal sensor placement</li>
					<li>Accelerated learning with ROMs</li>
				</ol>
			</section>

			<!-- Closed-loop active flow control -->
			<section>
				<section>
					<h2>Closed-loop active flow control</h2>
				</section>
				<section>
					<p>Goals of flow control:</p>
					<ul>
						<li>drag reduction</li>
						<li>load reduction</li>
						<li>process intensification</li>
						<li>noise reduction</li>
						<li>...</li>
					</ul>
				</section>
				<section>
					<p>Categories of flow control:</p>
					<ol>
						<li><b>passive:</b> modification of geometry, topology, fluid, ...</li>
						<li><b>active:</b> flow actuation through moving parts, blowing/suction, heating/cooling, ...</li>
					</ol>
					<p>Active flow control can be more effective but requires <span style="color: red;">energy</span>.</p>
				</section>
				<section>
					<p>Categories of active flow control:</p>
					<ol>
						<li><b>open-loop:</b> actuation prescript; constant or periodic motion, blowing, heating, ...</li>
						<li><b>closed-loop:</b> actuation based on sensor input</li>
					</ol>
					<p>Closed-loop flow control can be more effective but defining the control law is extremely challenging.</p>
				</section>
				<section>
					<video height="500" controls mute autoplay loop>
						<source src="images/of_conf_2021/variable_inflow_2x.mp4">
					</video>
					<p>Closed-loop flow control with variable Reynolds number; source: <a href="https://doi.org/10.5281/zenodo.5634050">F. Gabriel 2021</a>.</p>
				</section>
				<section>
					<p>How to find the control law?</p>
					<ul>
						<li>careful manual design</li>
						<li>adjoint optimization</li>
						<li>machine learning control (MLC)</li>
						<li><b>(deep) reinforcement learning (DRL)</b></li>
					</ul>
				</section>
				<section>
					<p>Favorable attributes of DRL:</p>
					<ul>
						<li>sample efficient thanks to NN-based function approximation</li>
						<li>discrete, continuous, or mixed states and actions</li>
						<li>control law is learnt from scratch</li>
						<li>can deal with uncertainty</li>
						<li>high degree of automation possible</li>
					</ul>
				</section>
				<section>
					<p>Why CFD-based closed-loop control via DRL?</p>
					<ul>
						<li>save virtual environment</li>
						<li>prior optimization, e.g., sensor placement</li>
					</ul>
					<p>Main challenge: CFD environments are expensive!</p>
				</section>


			</section>

			<!-- RL basics -->
			<section>
				<section>
					<h2>Reinforcement learning basics</h2>
				</section>
				<section>
					<img src="images/reinforcement_web.svg" alt="rl_overview"
									style="background:none; border:none; box-shadow:none; width: 600px">
					<p>Create an intelligent agent that learns to map <b>states</b> to <b>actions</b> such that cumulative rewards are maximized.</p>
				</section>
				<section>
					<img src="images/drl_feb_2023/cylinder_probes.png" alt="rl_overview"
									style="background:none; border:none; box-shadow:none; width: 100%">
					<p>Flow past a cylinder benchmark.</p>
				</section>
				<section>
					<p>Experience tuple:</p>
					<p>
						$$
						\left\{ S_t, A_t, R_{t+1}, S_{t+1}\right\}
						$$
					</p>
					<p>Trajectory:</p>
					<p>
						$ \left\{S_0, A_0, R_1, S_1\right\} $<br>
						$ \left\{S_1, A_1, R_2, S_3\right\} $<br>
						$\left\{ ...\right\} $
					</p>
				</section>
				<section>
					<img src="images/drl_feb_2023/cd_vs_eps.svg" alt="rl_overview"
									style="background:none; border:none; box-shadow:none; width: 100%">
				</section>
				<section>
					<img src="images/drl_feb_2023/r_vs_eps.svg" alt="rl_overview"
									style="background:none; border:none; box-shadow:none; width: 100%">
					<p>$r=3-(c_d + 0.1 |c_l|)$</p>
				</section>
				<section>
					<p>Long-term consequences:</p>
					<p>
						$$
							G_t = \sum\limits_{l=0}^{N_t-t} \gamma^l R_{t+l}
						$$
					</p>
					<ul>
						<li>$t$ - control time step</li>
						<li>$G_t$ - discounted return</li>
						<li>$\gamma$ - discount factor, typically $\gamma=0.99$</li>
						<li>$N_t$ - number of control steps</li>
					</ul>
				</section>
				<section>
					<img src="images/drl_feb_2023/return_vs_eps.svg" alt="rl_overview"
									style="background:none; border:none; box-shadow:none; width: 100%">
				</section>
				<section>
					<p>DRL learning objective:<br><b>maximize expected cumulative rewards.</b></p>
				</section>
			</section>

			<!-- PPO close look -->
			<section>
				<section>
					<h2>A close look at proximal policy optimization (PPO)</h2>
				</section>
				<section>
					<h3>Why PPO?</h3>
					<ul>
						<li>continuous and discrete actions spaces</li>
						<li><b>relatively</b> simple implementation</li>
						<li>restricted (robust) policy updates</li>
						<li>sample efficient</li>
						<li>...</li>
					</ul>
					<p>Refer to <a href="https://arxiv.org/abs/2006.11005">R. Paris et al. 2021</a> and the references therein for similar works employing PPO.</p>
				</section>
				<section>
					<img src="images/ofw_ssd/ppo_overview.png" alt="ppo_overview"
						style="background:none; border:none; box-shadow:none; width: 400px">
					<p>Proximal policy optimization (PPO) workflow (GAE - generalized advantage estimate).</p>
				</section>
				<section>
					<img src="images/ofw_ssd/policy_network.png" alt="ppo_overview"
						style="background:none; border:none; box-shadow:none; width: 500px">
					<p>Policy network predicts probability density function(s) for action(s).</p>
				</section>
				<section>
					<img src="images/DRL/gauss_vs_beta_dist.svg" alt="probes" width="80%" style="background:none; border:none; box-shadow:none;">
					<p>Comparison of Gauss and Beta distribution.</p>
				</section>
				<section>
					<p>learning what to expect in a given state - value function loss</p>
					<p>
						$$
						  L_V = \frac{1}{N_\tau N_t} \sum\limits_{\tau = 1}^{N_\tau}\sum\limits_{t = 1}^{N_t} \left( V(s_t^\tau) - G_t^\tau \right)^2
						$$
					</p>
					<ul>
						<li>$\tau$ - trajectory (single simulation)</li>
						<li>$s_t$ - state/observation (pressure)</li>
						<li>$V$ - parametrized value function</li>
						<li>clipping not included</li>
					</ul>
				</section>
				<section>
					<img src="images/drl_feb_2023/value_vs_eps.svg" alt="rl_overview"
									style="background:none; border:none; box-shadow:none; width: 100%">
				</section>
				<section>
					<p>Was the selected action a good one?</p>
					<p>
						$$\delta_t = R_t + \gamma V(s_{t+1}) - V(s_t) $$
						$$
						  A_t^{GAE} = \sum\limits_{l=0}^{N_t-t} (\gamma \lambda)^l \delta_{t+l}
						$$
					</p>
					<ul>
						<li>$\delta_t$ - one-step advantage estimate</li>
						<li>$A_t^{GAE}$ - generalized advantage estimate</li>
						<li>$\lambda$ - smoothing parameter</li>
					</ul>
				</section>
				<section>
					<img src="images/drl_feb_2023/gae_vs_eps.svg" alt="rl_overview"
									style="background:none; border:none; box-shadow:none; width: 100%">
				</section>
				<section>
					<p>make good actions more likely - policy objective function</p>
					<p>
						$$
						  J_\pi = \frac{1}{N_\tau N_t} \sum\limits_{\tau = 1}^{N_\tau}\sum\limits_{t = 1}^{N_t} \left( \frac{\pi(a_t|s_t)}{\pi^{old}(a_t|s_t)} A^{GAE,\tau}_t\right)
						$$
					</p>
					<ul>
						<li>$\pi$ - current policy</li>
						<li>$\pi^{old}$ - old policy (previous episode)</li>
						<li>clipping and entropy not included</li>
						<li>$J_\pi$ is <b>maximized</b></li>
					</ul>
				</section>
			</section>

			<!-- Optimal sensor placement -->
			<section>
				<section>
					<h2>Optimal sensor placement</h2>
					<p>
						Tom Krogmann, <a href="https://github.com/TomKrogmann/Optimal_Sensor_Placement_for_Active_Flow_Control_in_Deep_Reinforcement_Learning">Github</a>,
						<a href="https://zenodo.org/record/7636959">10.5281/zenodo.7636959</a>
					</p>
				</section>
				<section>
					<img src="images/drl_feb_2023/pinnball_setup.png" alt="rl_overview"
									style="background:none; border:none; box-shadow:none; width: 100%">
					<p>Fluidic pinnball setup.</p>
				</section>
				<section>
					<img src="images/drl_feb_2023/pinnball_cl_re.png" alt="rl_overview"
									style="background:none; border:none; box-shadow:none; width: 100%">
					<p>Mean lift $\mu_{c_L}$ over the Reynolds number $Re$.</p>
				</section>
				<section>
					<p>Challenge with optimal sensor placement and flow control:<br>
						<b>actuation changes the dynamical system</b></p>
				</section>
				<section>
					<p><b>Idea:</b> include sensor placement in DRL optimization via attention</p>
					<p><b>Attention:</b> encoder-decoder structure with softmax</p>
				</section>
				<section>
					<img src="images/drl_feb_2023/pinnball_attention.png" alt="rl_overview"
									style="background:none; border:none; box-shadow:none; width: 100%">
					<p>Time-averaged attention weights $\bar{\kappa}$.</p>
				</section>
				<section>
					<img src="images/drl_feb_2023/pinnball_optimal_sensors.png" alt="rl_overview"
									style="background:none; border:none; box-shadow:none; width: 100%">
					<p>Results obtained with top 7 sensors (MDI - mean decrease of impurity, modes - QR column pivoting).</p>
				</section>
			</section>

			<!-- Accelerated learning with ROMs -->
			<section>
				<section>
					<h2>Accelerated learning with reduced-order models (ROMs)</h2>
					<p>
						Janis Geise, <a href="https://github.com/JanisGeise/robust_MB_DRL_for_flow_control">Github</a>,
						<a href="https://zenodo.org/record/7642927">10.5281/zenodo.7642927</a>
					</p>
				</section>
				<section>
					<p><b>Idea:</b> replace CFD with ROM in regular intervals</p>
					<ul>
						<li>
							model ensemble
						</li>
						<li>~30 time delays</li>
						<li>fully-connected, feed-forward </li>
					</ul>
					<p>Challenge: <b>automated</b> creation of accurate models.</p>
				</section>
				<section>
					<img src="images/drl_feb_2023/rewards_vs_episode_l4.png" alt="rl_overview"
									style="background:none; border:none; box-shadow:none; width: 100%">
				</section>
				<section>
					<img src="images/drl_feb_2023/comparison_cl_cd.png" alt="rl_overview"
									style="background:none; border:none; box-shadow:none; width: 100%">
				</section>
				<section>
					<img src="images/drl_feb_2023/time_reduction.png" alt="rl_overview"
									style="background:none; border:none; box-shadow:none; width: 75%">
					<p>More time savings possible!</p>				
				</section>
			</section>

			<!-- end -->
			<section>
				<section>
					<h1>THE END</h1>
					<h3>Thank you for you attention!</h3>
					<img src="images/of_conf_2020/for2895_logo.png" alt="for2895"
						style="background:none; border:none; box-shadow:none; width: 70%">
					<div class="row_img">
						<div class="column_img">
							<div class="content_logo">
								<table>
									<tr>
										<td style="border: 0; padding-right: 5px;">&#9993;</th>
										<td style="border: 0; padding-right: 20px;"><a
												href="mailto:a.weiner@tu-braunschweig.de">Mail</a></th>
										<td style="border: 0; padding-right: 5px;">&#9741;</td>
										<td style="border: 0; padding-right: 0px;"><a
												href="https://www.linkedin.com/in/andre-weiner-a79752133/">LinkedIn</a></td>
										<td style="border: 0; padding-right: 5px;">&#9997;</td>
										<td style="border: 0; padding-right: 20px;"><a
												href="https://ml-cfd.com/">Blog</a></td>
										<td style="border: 0; padding-right: 5px;">&#10026;</td>
										<td style="border: 0; padding-right: 0px;"><a
												href="https://github.com/AndreWeiner">Github</a></td>
									</tr>
								</table>
							</div>
						</div>
					</div>
				</section>
			</section>

		</div>

	</div>

	<script src="js/reveal.js"></script>

	<script>

		// More info https://github.com/hakimel/reveal.js#configuration
		Reveal.initialize({
			controls: true,
			progress: true,
			center: true,
			hash: true,

			transition: 'slide', // none/fade/slide/convex/concave/zoom

			// More info https://github.com/hakimel/reveal.js#dependencies
			dependencies: [
				{ src: 'plugin/markdown/marked.js', condition: function () { return !!document.querySelector('[data-markdown]'); } },
				{ src: 'plugin/markdown/markdown.js', condition: function () { return !!document.querySelector('[data-markdown]'); } },
				{ src: 'plugin/highlight/highlight.js', async: true },
				{ src: 'plugin/search/search.js', async: true },
				{ src: 'plugin/zoom-js/zoom.js', async: true },
				{ src: 'plugin/notes/notes.js', async: true },
				{ src: 'plugin/math/math.js', async: true }]
		});
		Reveal.configure({ slideNumber: true });

	</script>


</body>

</html>